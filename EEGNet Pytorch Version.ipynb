{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from math import ceil,floor\n",
    "\n",
    "random_seed = 58\n",
    "n_class = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's the description from the paper</p>\n",
    "<img src=\"newEEGNet.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_channel , timestamps, F1, sample_rate, D, dropout_prob, n_class):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.n_class = n_class\n",
    "        self.D = D\n",
    "        self.F1 = F1\n",
    "        self.timestamps = timestamps\n",
    "        \n",
    "        # Block 1\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        #padding to implement mode = 'same'\n",
    "        self.padding_1 = nn.ZeroPad2d((sample_rate//4-1, sample_rate//4, 0, 0))\n",
    "        self.conv_1 = nn.Conv2d(1, F1, (1, sample_rate//2))\n",
    "        self.batchnorm_1 = nn.BatchNorm2d(F1, False)\n",
    "        self.depthwise_1 = nn.Conv2d(F1, D*F1, (n_channel,1), groups=F1)\n",
    "        self.batchnorm_2 = nn.BatchNorm2d(D*F1, False)\n",
    "        \n",
    "        # to reduce the sampling rate of the signal from 128 to 32\n",
    "        self.avgpool_1 = nn.AvgPool2d(1,4)\n",
    "        # Dropout\n",
    "        \n",
    "        #Block 2\n",
    "        \n",
    "        #Depthwise separable 2D convolution: Separable convolutions consist in first performing a depthwise spatial convolution\n",
    "        #(which acts on each input channel separately) followed by a pointwise convolution \n",
    "        #which mixes together the resulting output channels.\n",
    "        self.padding_2 = nn.ZeroPad2d((sample_rate//16-1, sample_rate//16, 0, 0))\n",
    "        self.seperate_1 = nn.Conv2d(D*F1, D*F1, (1, sample_rate//8),groups=F1*D,bias=False)\n",
    "        self.seperate_2 = nn.Conv2d(D*F1, D*F1, 1,bias=False)\n",
    "        \n",
    "        self.batchnorm_3 = nn.BatchNorm2d(D*F1, False)\n",
    "        self.avgpool_2 = nn.AvgPool2d(1,8)\n",
    "\n",
    "        #FC Layer\n",
    "        self.fc1 = nn.Linear(1000,n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Block 1\n",
    "        x = self.padding_1(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batchnorm_1(x)\n",
    "        \n",
    "        x = self.depthwise_1(x)\n",
    "        \n",
    "        x = self.batchnorm_2(x)\n",
    "        \n",
    "        x = F.elu(x)\n",
    "        x = self.avgpool_1(x)\n",
    "        x = F.dropout(x, self.dropout_prob)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.padding_2(x)\n",
    "        \n",
    "        x = self.seperate_1(x)\n",
    "        x = self.seperate_2(x)\n",
    "        #print (x.shape)\n",
    "        x = self.batchnorm_3(x)\n",
    "        \n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.avgpool_2(x)\n",
    "        \n",
    "        x = F.dropout(x, self.dropout_prob)\n",
    "        \n",
    "        \n",
    "        #FC Layer\n",
    "        # 16 depends on x.size()\n",
    "        #print (x.shape)\n",
    "        x = x.view(-1,1000)\n",
    "        # sigmoid for binary; softmax for multi classes\n",
    "        x = F.softmax(self.fc1(x),dim = 1)\n",
    "        \n",
    "        return x       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGNet(n_channel = 59 , timestamps=8000, F1=4, sample_rate=1000, D=1, dropout_prob=0.25, n_class=n_class)\n",
    "#a = torch.Tensor(np.random.rand(1, 1, 59, 8000))\n",
    "#outputs = model(a)\n",
    "#print (outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, params):\n",
    "    results = []\n",
    "    batch_size = 64\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(ceil(len(X)/batch_size)):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "        pred = model(inputs)\n",
    "        predicted.append(pred.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    inputs = Variable(torch.from_numpy(X))\n",
    "    outputs = model(inputs) \n",
    "    #predicted = predicted.data.cpu().numpy()\n",
    "\n",
    "    for param in params:\n",
    "        if param == \"acc\":\n",
    "            results.append(accuracy_score(Y, np.argmax(outputs.detach().cpu().numpy(),axis = 1)))\n",
    "#         if param == \"recall\":\n",
    "#             results.append(recall_score(Y, np.round(predicted)))\n",
    "#         if param == \"precision\":\n",
    "#             results.append(precision_score(Y, np.round(predicted)))\n",
    "        if param == \"f1\":\n",
    "            results.append(f1_score(Y, np.argmax(outputs.detach().cpu().numpy(),axis = 1)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #(#samples, 1, #channel,timeseries)\n",
    "#X_train = np.random.rand(134, 1, 59, 8000).astype('float32') # np.random.rand generates between [0, 1)\n",
    "#Y_train = np.round(np.random.rand(134,n_class).astype('float32')) # binary data, so we round it to 0 or 1.\n",
    "\n",
    "# X_val = np.random.rand(100, 1, 64, 120).astype('float32')\n",
    "# y_val = np.round(np.random.rand(100,n_class).astype('float32'))\n",
    "\n",
    "# X_test = np.random.rand(100, 1, 64, 120).astype('float32')\n",
    "# y_test = np.round(np.random.rand(100,n_class).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from numpy import newaxis\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def Process_Data(mypath):\n",
    "    data = []\n",
    "    Y  = []\n",
    "    datafiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    for d in datafiles:   \n",
    "        mat = scipy.io.loadmat(mypath+d)\n",
    "        MrkPos= np.array(mat['mrk'][0,0][0])\n",
    "        MrkLable = np.array(mat['mrk'][0,0][1])\n",
    "        Raw_Data = np.array(mat['cnt']).T\n",
    "\n",
    "        # calculate time duration for each EEG\n",
    "        Time_diff = []\n",
    "        for i in range(1,MrkPos.shape[1]):\n",
    "            Time_diff.append(MrkPos[0,i] - MrkPos[0,i-1])\n",
    "\n",
    "        #parsing Data\n",
    "\n",
    "        for i in range(1,MrkPos.shape[1]):\n",
    "            if (MrkPos[0,i] - MrkPos[0,i-1] == 8000):\n",
    "                tmp_sample = Raw_Data[newaxis, :, MrkPos[0,i-1]:MrkPos[0,i]]\n",
    "                data.append(tmp_sample)\n",
    "                if (MrkLable[0,i] == -1):\n",
    "                    Y.append(0)\n",
    "                else:\n",
    "                    Y.append(1)\n",
    "\n",
    "    #append 3-D samples to 4-D (#samples, 1, #timepoints, #channels) \n",
    "    X_train = np.concatenate([arr[np.newaxis] for arr in data]).astype('float32') \n",
    "    Y_train = np.array(Y).reshape(-1).astype('float32')\n",
    "    return X_train,Y_train;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = Process_Data('./data/BCICIV_1calib_1000Hz_mat/') \n",
    "np.random.shuffle(X)\n",
    "np.random.shuffle(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val,Y_val = Process_Data('./data/BCICIV_1eval_1000Hz_mat/')\n",
    "\n",
    "X_train = X[0:900]\n",
    "Y_train = Y[0:900]\n",
    "\n",
    "X_test = X[901:]\n",
    "Y_test = Y[901:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59,)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0\n",
      "Batch  0\n",
      "Batch  1\n",
      "Batch  2\n",
      "Batch  3\n",
      "Batch  4\n",
      "Batch  5\n",
      "Batch  6\n",
      "Batch  7\n",
      "Batch  8\n",
      "Batch  9\n",
      "Batch  10\n",
      "Batch  11\n",
      "Batch  12\n",
      "Batch  13\n",
      "['f1', 'acc']\n",
      "Training Loss  9.73785412311554\n",
      "Train -  [0.5476923076923077, 0.51]\n",
      "Validation -  [0.4333333333333333, 0.423728813559322]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    print (\"\\nEpoch \", epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(ceil(len(X_train)/batch_size)-1):\n",
    "        print (\"Batch \", i)\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = torch.from_numpy(X_train[s:e])\n",
    "        labels = torch.LongTensor(Y_train[s:e].T)\n",
    "        #labels = Y_train[:,s:e].reshape(-1)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        #print(\"output \",outputs.size(),\"labels \",labels.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation accuracy\n",
    "    params = [\"f1\",\"acc\"]\n",
    "    print (params)\n",
    "    print (\"Training Loss \", running_loss)\n",
    "    #print (\"Train - \", evaluate(model, X_train, Y_train, params))\n",
    "    print (\"Validation - \", evaluate(model, X_test, Y_test, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(outputs.detach().cpu().numpy(),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     model.eval()\n",
    "#     y_pred = np.array([])\n",
    "#     y_true = np.array([])\n",
    "#     inputs = torch.from_numpy(X_val)\n",
    "#     labels = torch.FloatTensor(np.array([y_val]).T)\n",
    "        \n",
    "#     # wrap them in Variable\n",
    "#     inputs, labels = Variable(inputs), Variable(labels)\n",
    "#     outputs = model(inputs)\n",
    "    \n",
    "\n",
    "#     y_pred = np.append(y_pred, outputs.detach().numpy().reshape(-1))\n",
    "#     y_true = np.append(y_true, labels.numpy().reshape(-1))\n",
    "    \n",
    "    #print(y_pred.size,y_true.size)\n",
    "    #print(\"Dev F1: \", f1_score(y_true, y_pred, average='macro'))\n",
    "    #f1s.append(f1_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timepoint = np.array([MrkPos[0,i]-MrkPos[0,i-1] for i in range(1,MrkPos.shape[1])])\n",
    "# timepoint = np.reshape(timepoint, (-1, len(timepoint)))\n",
    "# timepoint = np.insert(timepoint,0,MrkPos[0,0],axis =1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
