{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from braindecode.datasets.bcic_iv_2a import BCICompetition4Set2A\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, \\\n",
    "    RuntimeMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or\n",
    "from braindecode.datautil.iterators import BalancedBatchSizeIterator\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.datautil.splitters import split_into_two_sets\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "from braindecode.torch_ext.util import set_random_seeds, np_to_var\n",
    "from braindecode.mne_ext.signalproc import mne_apply\n",
    "from braindecode.datautil.signalproc import (bandpass_cnt,\n",
    "                                             exponential_running_standardize)\n",
    "from braindecode.datautil.trial_segment import create_signal_target_from_raw_mne\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's the description from the paper</p>\n",
    "<img src=\"DeepConvNet.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's the description from the paper</p>\n",
    "<img src=\"DeepConvNetDetail.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(data_folder, subject_id, low_cut_hz, model, cuda):\n",
    "    ival = [-500, 4000]\n",
    "    max_epochs = 1600\n",
    "    max_increase_epochs = 160\n",
    "    batch_size = 60\n",
    "    high_cut_hz = 38\n",
    "    factor_new = 1e-3\n",
    "    init_block_size = 1000\n",
    "    valid_set_fraction = 0.2\n",
    "\n",
    "    train_filename = 'A{:02d}T.gdf'.format(subject_id)\n",
    "    test_filename = 'A{:02d}E.gdf'.format(subject_id)\n",
    "    train_filepath = os.path.join(data_folder, train_filename)\n",
    "    test_filepath = os.path.join(data_folder, test_filename)\n",
    "    train_label_filepath = train_filepath.replace('.gdf', '.mat')\n",
    "    test_label_filepath = test_filepath.replace('.gdf', '.mat')\n",
    "\n",
    "    train_loader = BCICompetition4Set2A(\n",
    "        train_filepath, labels_filename=train_label_filepath)\n",
    "    test_loader = BCICompetition4Set2A(\n",
    "        test_filepath, labels_filename=test_label_filepath)\n",
    "    train_cnt = train_loader.load()\n",
    "    test_cnt = test_loader.load()\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    train_cnt = train_cnt.drop_channels(['STI 014', 'EOG-left',\n",
    "                                         'EOG-central', 'EOG-right'])\n",
    "    assert len(train_cnt.ch_names) == 22\n",
    "    # lets convert to millvolt for numerical stability of next operations\n",
    "    train_cnt = mne_apply(lambda a: a * 1e6, train_cnt)\n",
    "    train_cnt = mne_apply(\n",
    "        lambda a: bandpass_cnt(a, low_cut_hz, high_cut_hz, train_cnt.info['sfreq'],\n",
    "                               filt_order=3,\n",
    "                               axis=1), train_cnt)\n",
    "    train_cnt = mne_apply(\n",
    "        lambda a: exponential_running_standardize(a.T, factor_new=factor_new,\n",
    "                                                  init_block_size=init_block_size,\n",
    "                                                  eps=1e-4).T,\n",
    "        train_cnt)\n",
    "\n",
    "    test_cnt = test_cnt.drop_channels(['STI 014', 'EOG-left',\n",
    "                                       'EOG-central', 'EOG-right'])\n",
    "    assert len(test_cnt.ch_names) == 22\n",
    "    test_cnt = mne_apply(lambda a: a * 1e6, test_cnt)\n",
    "    test_cnt = mne_apply(\n",
    "        lambda a: bandpass_cnt(a, low_cut_hz, high_cut_hz, test_cnt.info['sfreq'],\n",
    "                               filt_order=3,\n",
    "                               axis=1), test_cnt)\n",
    "    test_cnt = mne_apply(\n",
    "        lambda a: exponential_running_standardize(a.T, factor_new=factor_new,\n",
    "                                                  init_block_size=init_block_size,\n",
    "                                                  eps=1e-4).T,\n",
    "        test_cnt)\n",
    "\n",
    "    marker_def = OrderedDict([('Left Hand', [1]), ('Right Hand', [2],),\n",
    "                              ('Foot', [3]), ('Tongue', [4])])\n",
    "\n",
    "    train_set = create_signal_target_from_raw_mne(train_cnt, marker_def, ival)\n",
    "    test_set = create_signal_target_from_raw_mne(test_cnt, marker_def, ival)\n",
    "\n",
    "    train_set, valid_set = split_into_two_sets(\n",
    "        train_set, first_set_fraction=1-valid_set_fraction)\n",
    "\n",
    "    set_random_seeds(seed=20190706, cuda=cuda)\n",
    "\n",
    "    n_classes = 4\n",
    "    n_chans = int(train_set.X.shape[1])\n",
    "    input_time_length = train_set.X.shape[2]\n",
    "    if model == 'shallow':\n",
    "        model = ShallowFBCSPNet(n_chans, n_classes, input_time_length=input_time_length,\n",
    "                            final_conv_length='auto').create_network()\n",
    "    elif model == 'deep':\n",
    "        model = Deep4Net(n_chans, n_classes, input_time_length=input_time_length,\n",
    "                            final_conv_length='auto').create_network()\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    log.info(\"Model: \\n{:s}\".format(str(model)))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    iterator = BalancedBatchSizeIterator(batch_size=batch_size)\n",
    "\n",
    "    stop_criterion = Or([MaxEpochs(max_epochs),\n",
    "                         NoDecrease('valid_misclass', max_increase_epochs)])\n",
    "\n",
    "    monitors = [LossMonitor(), MisclassMonitor(), RuntimeMonitor()]\n",
    "\n",
    "    model_constraint = MaxNormDefaultConstraint()\n",
    "\n",
    "    exp = Experiment(model, train_set, valid_set, test_set, iterator=iterator,\n",
    "                     loss_function=F.nll_loss, optimizer=optimizer,\n",
    "                     model_constraint=model_constraint,\n",
    "                     monitors=monitors,\n",
    "                     stop_criterion=stop_criterion,\n",
    "                     remember_best_column='valid_misclass',\n",
    "                     run_after_early_stop=True, cuda=cuda)\n",
    "    exp.run()\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from ./data/BCICIV_2a_gdf/A01T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 672527  =      0.000 ...  2690.108 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/mne/io/edf/edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n",
      "/anaconda3/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Overlapping events detected. Use find_edf_events for the original events.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/anaconda3/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Interpolating stim channel. Events may jitter.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/anaconda3/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from ./data/BCICIV_2a_gdf/A01E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 686999  =      0.000 ...  2747.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/mne/io/edf/edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n",
      "/anaconda3/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Overlapping events detected. Use find_edf_events for the original events.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/anaconda3/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Interpolating stim channel. Events may jitter.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/anaconda3/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-12 20:51:21,288 INFO : Trial per class:\n",
      "Counter({'Tongue': 72, 'Foot': 72, 'Right Hand': 72, 'Left Hand': 72})\n",
      "2019-02-12 20:51:21,598 INFO : Trial per class:\n",
      "Counter({'Left Hand': 72, 'Right Hand': 72, 'Foot': 72, 'Tongue': 72})\n",
      "2019-02-12 20:51:21,895 INFO : Model: \n",
      "Sequential(\n",
      "  (dimshuffle): Expression(expression=_transpose_time_to_spat)\n",
      "  (conv_time): Conv2d(1, 25, kernel_size=(10, 1), stride=(1, 1))\n",
      "  (conv_spat): Conv2d(25, 25, kernel_size=(1, 22), stride=(1, 1), bias=False)\n",
      "  (bnorm): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_nonlin): Expression(expression=elu)\n",
      "  (pool): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_nonlin): Expression(expression=identity)\n",
      "  (drop_2): Dropout(p=0.5)\n",
      "  (conv_2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm_2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (nonlin_2): Expression(expression=elu)\n",
      "  (pool_2): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_nonlin_2): Expression(expression=identity)\n",
      "  (drop_3): Dropout(p=0.5)\n",
      "  (conv_3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm_3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (nonlin_3): Expression(expression=elu)\n",
      "  (pool_3): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_nonlin_3): Expression(expression=identity)\n",
      "  (drop_4): Dropout(p=0.5)\n",
      "  (conv_4): Conv2d(100, 200, kernel_size=(10, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm_4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (nonlin_4): Expression(expression=elu)\n",
      "  (pool_4): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_nonlin_4): Expression(expression=identity)\n",
      "  (conv_classifier): Conv2d(200, 4, kernel_size=(9, 1), stride=(1, 1))\n",
      "  (softmax): LogSoftmax()\n",
      "  (squeeze): Expression(expression=_squeeze_final_output)\n",
      ")\n",
      "2019-02-12 20:51:21,897 INFO : Run until first stop...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c77c4288e872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'deep'\u001b[0m \u001b[0;31m#'shallow' or 'deep'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cut_hz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Last 10 epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3d9c8bebd409>\u001b[0m in \u001b[0;36mrun_exp\u001b[0;34m(data_folder, subject_id, low_cut_hz, model, cuda)\u001b[0m\n\u001b[1;32m     96\u001b[0m                      \u001b[0mremember_best_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid_misclass'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                      run_after_early_stop=True, cuda=cuda)\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/braindecode/experiments/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run until first stop...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_first_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_early_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# always setup for second stop, in order to get best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/braindecode/experiments/experiment.py\u001b[0m in \u001b[0;36mrun_until_first_stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0muntil\u001b[0m \u001b[0mstop\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfulfilled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremember_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_early_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_until_second_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/braindecode/experiments/experiment.py\u001b[0m in \u001b[0;36mrun_until_stop\u001b[0;34m(self, datasets, remember_best)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_0_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremember_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/braindecode/experiments/experiment.py\u001b[0m in \u001b[0;36mmonitor_epoch\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m                 \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/braindecode/experiments/experiment.py\u001b[0m in \u001b[0;36meval_on_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0minput_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mtarget_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                    level=logging.DEBUG, stream=sys.stdout)\n",
    "# Should contain both .gdf files and .mat-labelfiles from competition\n",
    "data_folder = './data/BCICIV_2a_gdf/'\n",
    "subject_id = 1 # 1-9\n",
    "low_cut_hz = 4 # 0 or 4\n",
    "model = 'deep' #'shallow' or 'deep'\n",
    "cuda = False\n",
    "exp = run_exp(data_folder, subject_id, low_cut_hz, model, cuda)\n",
    "log.info(\"Last 10 epochs\")\n",
    "log.info(\"\\n\" + str(exp.epochs_df.iloc[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data/BCICIV_2a_gdf/'\n",
    "train_filename = 'A{:02d}T.gdf'.format(subject_id)\n",
    "test_filename = 'A{:02d}E.gdf'.format(subject_id)\n",
    "train_filepath = os.path.join(data_folder, train_filename)\n",
    "test_filepath = os.path.join(data_folder, test_filename)\n",
    "train_label_filepath = train_filepath.replace('.gdf', '.mat')\n",
    "test_label_filepath = test_filepath.replace('.gdf', '.mat')\n",
    "train_loader = BCICompetition4Set2A(\n",
    "        train_filepath, labels_filename=train_label_filepath)\n",
    "\n",
    "# cnt = self.extract_data()\n",
    "# events, artifact_trial_mask = self.extract_events(cnt)\n",
    "# cnt.info['events'] = events\n",
    "# cnt.info['artifact_trial_mask'] = artifact_trial_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from ./data/BCICIV_2a_gdf/A01T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 672527  =      0.000 ...  2690.108 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/mne/io/edf/edf.py:1083: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n",
      "<ipython-input-5-a1842a4aa3d7>:2: RuntimeWarning: Overlapping events detected. Use find_edf_events for the original events.\n",
      "  raw_edf = mne.io.read_raw_edf(train_filepath, stim_channel='auto')\n",
      "<ipython-input-5-a1842a4aa3d7>:2: RuntimeWarning: Interpolating stim channel. Events may jitter.\n",
      "  raw_edf = mne.io.read_raw_edf(train_filepath, stim_channel='auto')\n",
      "<ipython-input-5-a1842a4aa3d7>:2: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  raw_edf = mne.io.read_raw_edf(train_filepath, stim_channel='auto')\n"
     ]
    }
   ],
   "source": [
    "Stop\n",
    "import mne\n",
    "raw_edf = mne.io.read_raw_edf(train_filepath, stim_channel='auto')\n",
    "raw_edf.load_data()\n",
    "data = raw_edf.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Info | 16 non-empty fields\n",
       "    bads : list | 0 items\n",
       "    ch_names : list | EEG-Fz, EEG-0, EEG-1, EEG-2, EEG-3, EEG-4, EEG-5, ...\n",
       "    chs : list | 26 items (EEG: 25, STIM: 1)\n",
       "    comps : list | 0 items\n",
       "    custom_ref_applied : bool | False\n",
       "    dev_head_t : Transform | 3 items\n",
       "    events : list | 0 items\n",
       "    highpass : float | 0.5 Hz\n",
       "    hpi_meas : list | 0 items\n",
       "    hpi_results : list | 0 items\n",
       "    lowpass : float | 100.0 Hz\n",
       "    meas_date : tuple | 2005-01-17 12:00:00 GMT\n",
       "    nchan : int | 26\n",
       "    proc_history : list | 0 items\n",
       "    projs : list | 0 items\n",
       "    sfreq : float | 250.0 Hz\n",
       "    acq_pars : NoneType\n",
       "    acq_stim : NoneType\n",
       "    ctf_head_t : NoneType\n",
       "    description : NoneType\n",
       "    dev_ctf_t : NoneType\n",
       "    dig : NoneType\n",
       "    experimenter : NoneType\n",
       "    file_id : NoneType\n",
       "    gantry_angle : NoneType\n",
       "    hpi_subsystem : NoneType\n",
       "    kit_system_id : NoneType\n",
       "    line_freq : NoneType\n",
       "    meas_id : NoneType\n",
       "    proj_id : NoneType\n",
       "    proj_name : NoneType\n",
       "    subject_info : NoneType\n",
       "    xplotter_layout : NoneType\n",
       ">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_edf.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x132870518>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-12 20:34:12,528 DEBUG : update_title_pos\n",
      "2019-02-12 20:34:12,565 DEBUG : update_title_pos\n",
      "2019-02-12 20:34:12,596 DEBUG : update_title_pos\n",
      "2019-02-12 20:34:12,629 DEBUG : update_title_pos\n",
      "2019-02-12 20:34:12,647 DEBUG : update_title_pos\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(exp.epochs_df['test_misclass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_misclass</th>\n",
       "      <th>valid_misclass</th>\n",
       "      <th>test_misclass</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.441879</td>\n",
       "      <td>1.405375</td>\n",
       "      <td>1.429315</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.753472</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.368649</td>\n",
       "      <td>1.380442</td>\n",
       "      <td>1.369595</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.704861</td>\n",
       "      <td>8.121811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.530694</td>\n",
       "      <td>1.588478</td>\n",
       "      <td>1.535021</td>\n",
       "      <td>0.743478</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.020666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.896133</td>\n",
       "      <td>2.025539</td>\n",
       "      <td>1.907914</td>\n",
       "      <td>0.682609</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>8.232728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.532888</td>\n",
       "      <td>2.743549</td>\n",
       "      <td>2.551060</td>\n",
       "      <td>0.743478</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.274704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.979870</td>\n",
       "      <td>3.259540</td>\n",
       "      <td>2.998601</td>\n",
       "      <td>0.743478</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.354777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.104247</td>\n",
       "      <td>3.489383</td>\n",
       "      <td>3.141852</td>\n",
       "      <td>0.743478</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.245589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.068217</td>\n",
       "      <td>3.534096</td>\n",
       "      <td>3.105667</td>\n",
       "      <td>0.743478</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.052042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.002282</td>\n",
       "      <td>3.537081</td>\n",
       "      <td>3.039664</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.746528</td>\n",
       "      <td>8.363468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.806259</td>\n",
       "      <td>3.397973</td>\n",
       "      <td>2.863057</td>\n",
       "      <td>0.721739</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>8.292935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.684402</td>\n",
       "      <td>3.277142</td>\n",
       "      <td>2.739682</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.725694</td>\n",
       "      <td>7.937675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.680008</td>\n",
       "      <td>3.294525</td>\n",
       "      <td>2.761077</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>8.006284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.433926</td>\n",
       "      <td>3.060590</td>\n",
       "      <td>2.580139</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.711806</td>\n",
       "      <td>7.929194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.356114</td>\n",
       "      <td>2.979027</td>\n",
       "      <td>2.527742</td>\n",
       "      <td>0.691304</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>8.072255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.344212</td>\n",
       "      <td>2.950974</td>\n",
       "      <td>2.521890</td>\n",
       "      <td>0.686957</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.704861</td>\n",
       "      <td>7.998901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.311657</td>\n",
       "      <td>2.918518</td>\n",
       "      <td>2.501140</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.704861</td>\n",
       "      <td>8.025282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.188965</td>\n",
       "      <td>2.773315</td>\n",
       "      <td>2.379953</td>\n",
       "      <td>0.669565</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>7.876470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.074629</td>\n",
       "      <td>2.641801</td>\n",
       "      <td>2.263226</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>7.927263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.892167</td>\n",
       "      <td>2.474407</td>\n",
       "      <td>2.106016</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.280479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.936568</td>\n",
       "      <td>2.562576</td>\n",
       "      <td>2.157722</td>\n",
       "      <td>0.656522</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.264108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.065490</td>\n",
       "      <td>2.775990</td>\n",
       "      <td>2.299773</td>\n",
       "      <td>0.647826</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>6.229817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.922781</td>\n",
       "      <td>2.644304</td>\n",
       "      <td>2.184341</td>\n",
       "      <td>0.626087</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.350921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.648363</td>\n",
       "      <td>2.332223</td>\n",
       "      <td>1.942371</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>6.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.886910</td>\n",
       "      <td>2.592978</td>\n",
       "      <td>2.156252</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.649306</td>\n",
       "      <td>6.322739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.894080</td>\n",
       "      <td>2.628870</td>\n",
       "      <td>2.156801</td>\n",
       "      <td>0.639130</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>6.288002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.869447</td>\n",
       "      <td>2.628780</td>\n",
       "      <td>2.136935</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>6.286578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.847853</td>\n",
       "      <td>2.591529</td>\n",
       "      <td>2.125970</td>\n",
       "      <td>0.621739</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>6.360933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.717492</td>\n",
       "      <td>2.455853</td>\n",
       "      <td>2.017844</td>\n",
       "      <td>0.595652</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.621528</td>\n",
       "      <td>6.305207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.801834</td>\n",
       "      <td>2.580056</td>\n",
       "      <td>2.111988</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>6.271463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.830338</td>\n",
       "      <td>2.643034</td>\n",
       "      <td>2.144135</td>\n",
       "      <td>0.604348</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.642361</td>\n",
       "      <td>6.292075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.720066</td>\n",
       "      <td>1.497772</td>\n",
       "      <td>1.401841</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>10.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.678984</td>\n",
       "      <td>1.374029</td>\n",
       "      <td>1.329564</td>\n",
       "      <td>0.246528</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>10.321460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.586512</td>\n",
       "      <td>1.298434</td>\n",
       "      <td>1.237851</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>10.429427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.651747</td>\n",
       "      <td>1.250453</td>\n",
       "      <td>1.289527</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>10.275648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.517346</td>\n",
       "      <td>0.969094</td>\n",
       "      <td>1.093727</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.402778</td>\n",
       "      <td>10.308085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.539854</td>\n",
       "      <td>0.997077</td>\n",
       "      <td>1.168367</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>10.306196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.435593</td>\n",
       "      <td>0.881821</td>\n",
       "      <td>1.042820</td>\n",
       "      <td>0.173611</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>10.195339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.441343</td>\n",
       "      <td>0.837679</td>\n",
       "      <td>1.070325</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>10.387146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.587771</td>\n",
       "      <td>0.982054</td>\n",
       "      <td>1.250240</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>10.207377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.512586</td>\n",
       "      <td>0.950595</td>\n",
       "      <td>1.158567</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>10.324717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.360291</td>\n",
       "      <td>0.780645</td>\n",
       "      <td>1.038696</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>10.300213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.351780</td>\n",
       "      <td>0.724145</td>\n",
       "      <td>1.057757</td>\n",
       "      <td>0.149306</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>10.239567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.348796</td>\n",
       "      <td>0.643827</td>\n",
       "      <td>1.019051</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>10.293681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.404671</td>\n",
       "      <td>0.730122</td>\n",
       "      <td>1.083637</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.413194</td>\n",
       "      <td>10.267694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.377772</td>\n",
       "      <td>0.733600</td>\n",
       "      <td>1.097592</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>10.140549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.332467</td>\n",
       "      <td>0.642787</td>\n",
       "      <td>1.066131</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>10.221695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.306816</td>\n",
       "      <td>0.539793</td>\n",
       "      <td>1.024242</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.381944</td>\n",
       "      <td>10.227983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.239402</td>\n",
       "      <td>0.442503</td>\n",
       "      <td>0.939801</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.364583</td>\n",
       "      <td>10.172771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.236304</td>\n",
       "      <td>0.472072</td>\n",
       "      <td>0.977309</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.402778</td>\n",
       "      <td>10.241439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.303801</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.087420</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>10.352921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.287558</td>\n",
       "      <td>0.493353</td>\n",
       "      <td>1.013832</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>10.453943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.372596</td>\n",
       "      <td>0.613294</td>\n",
       "      <td>1.155101</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>10.337733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.408682</td>\n",
       "      <td>0.670880</td>\n",
       "      <td>1.178357</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.434028</td>\n",
       "      <td>10.259551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.383071</td>\n",
       "      <td>0.660548</td>\n",
       "      <td>1.149375</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.413194</td>\n",
       "      <td>10.325859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.369860</td>\n",
       "      <td>0.608254</td>\n",
       "      <td>1.176800</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>10.394575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.258705</td>\n",
       "      <td>0.406204</td>\n",
       "      <td>0.968001</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.399306</td>\n",
       "      <td>10.234006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.253313</td>\n",
       "      <td>0.397859</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.128472</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>10.217508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.385228</td>\n",
       "      <td>0.639438</td>\n",
       "      <td>1.186642</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>10.330666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.214645</td>\n",
       "      <td>0.386277</td>\n",
       "      <td>0.954328</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.392361</td>\n",
       "      <td>1721.873848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.098562</td>\n",
       "      <td>0.158942</td>\n",
       "      <td>0.833927</td>\n",
       "      <td>0.017361</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>40.783800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_loss  valid_loss  test_loss  train_misclass  valid_misclass  \\\n",
       "0      1.441879    1.405375   1.429315        0.747826        0.775862   \n",
       "1      1.368649    1.380442   1.369595        0.717391        0.775862   \n",
       "2      1.530694    1.588478   1.535021        0.743478        0.775862   \n",
       "3      1.896133    2.025539   1.907914        0.682609        0.793103   \n",
       "4      2.532888    2.743549   2.551060        0.743478        0.775862   \n",
       "5      2.979870    3.259540   2.998601        0.743478        0.775862   \n",
       "6      3.104247    3.489383   3.141852        0.743478        0.775862   \n",
       "7      3.068217    3.534096   3.105667        0.743478        0.775862   \n",
       "8      3.002282    3.537081   3.039664        0.739130        0.775862   \n",
       "9      2.806259    3.397973   2.863057        0.721739        0.775862   \n",
       "10     2.684402    3.277142   2.739682        0.713043        0.775862   \n",
       "11     2.680008    3.294525   2.761077        0.713043        0.775862   \n",
       "12     2.433926    3.060590   2.580139        0.700000        0.775862   \n",
       "13     2.356114    2.979027   2.527742        0.691304        0.775862   \n",
       "14     2.344212    2.950974   2.521890        0.686957        0.775862   \n",
       "15     2.311657    2.918518   2.501140        0.678261        0.775862   \n",
       "16     2.188965    2.773315   2.379953        0.669565        0.758621   \n",
       "17     2.074629    2.641801   2.263226        0.652174        0.741379   \n",
       "18     1.892167    2.474407   2.106016        0.630435        0.724138   \n",
       "19     1.936568    2.562576   2.157722        0.656522        0.741379   \n",
       "20     2.065490    2.775990   2.299773        0.647826        0.741379   \n",
       "21     1.922781    2.644304   2.184341        0.626087        0.724138   \n",
       "22     1.648363    2.332223   1.942371        0.591304        0.706897   \n",
       "23     1.886910    2.592978   2.156252        0.630435        0.758621   \n",
       "24     1.894080    2.628870   2.156801        0.639130        0.741379   \n",
       "25     1.869447    2.628780   2.136935        0.630435        0.741379   \n",
       "26     1.847853    2.591529   2.125970        0.621739        0.741379   \n",
       "27     1.717492    2.455853   2.017844        0.595652        0.741379   \n",
       "28     1.801834    2.580056   2.111988        0.600000        0.741379   \n",
       "29     1.830338    2.643034   2.144135        0.604348        0.724138   \n",
       "..          ...         ...        ...             ...             ...   \n",
       "94     0.720066    1.497772   1.401841        0.284722        0.465517   \n",
       "95     0.678984    1.374029   1.329564        0.246528        0.431034   \n",
       "96     0.586512    1.298434   1.237851        0.225694        0.413793   \n",
       "97     0.651747    1.250453   1.289527        0.270833        0.465517   \n",
       "98     0.517346    0.969094   1.093727        0.208333        0.310345   \n",
       "99     0.539854    0.997077   1.168367        0.218750        0.396552   \n",
       "100    0.435593    0.881821   1.042820        0.173611        0.344828   \n",
       "101    0.441343    0.837679   1.070325        0.194444        0.310345   \n",
       "102    0.587771    0.982054   1.250240        0.218750        0.344828   \n",
       "103    0.512586    0.950595   1.158567        0.218750        0.396552   \n",
       "104    0.360291    0.780645   1.038696        0.156250        0.344828   \n",
       "105    0.351780    0.724145   1.057757        0.149306        0.344828   \n",
       "106    0.348796    0.643827   1.019051        0.159722        0.275862   \n",
       "107    0.404671    0.730122   1.083637        0.166667        0.293103   \n",
       "108    0.377772    0.733600   1.097592        0.166667        0.344828   \n",
       "109    0.332467    0.642787   1.066131        0.159722        0.327586   \n",
       "110    0.306816    0.539793   1.024242        0.131944        0.258621   \n",
       "111    0.239402    0.442503   0.939801        0.111111        0.241379   \n",
       "112    0.236304    0.472072   0.977309        0.100694        0.241379   \n",
       "113    0.303801    0.553191   1.087420        0.135417        0.275862   \n",
       "114    0.287558    0.493353   1.013832        0.121528        0.224138   \n",
       "115    0.372596    0.613294   1.155101        0.159722        0.258621   \n",
       "116    0.408682    0.670880   1.178357        0.166667        0.258621   \n",
       "117    0.383071    0.660548   1.149375        0.152778        0.241379   \n",
       "118    0.369860    0.608254   1.176800        0.156250        0.310345   \n",
       "119    0.258705    0.406204   0.968001        0.114583        0.206897   \n",
       "120    0.253313    0.397859   0.976564        0.128472        0.189655   \n",
       "121    0.385228    0.639438   1.186642        0.177083        0.327586   \n",
       "122    0.214645    0.386277   0.954328        0.076389        0.155172   \n",
       "123    0.098562    0.158942   0.833927        0.017361        0.034483   \n",
       "\n",
       "     test_misclass      runtime  \n",
       "0         0.753472     0.000000  \n",
       "1         0.704861     8.121811  \n",
       "2         0.750000     8.020666  \n",
       "3         0.718750     8.232728  \n",
       "4         0.750000     8.274704  \n",
       "5         0.750000     8.354777  \n",
       "6         0.750000     8.245589  \n",
       "7         0.750000     8.052042  \n",
       "8         0.746528     8.363468  \n",
       "9         0.739583     8.292935  \n",
       "10        0.725694     7.937675  \n",
       "11        0.722222     8.006284  \n",
       "12        0.711806     7.929194  \n",
       "13        0.708333     8.072255  \n",
       "14        0.704861     7.998901  \n",
       "15        0.704861     8.025282  \n",
       "16        0.697917     7.876470  \n",
       "17        0.680556     7.927263  \n",
       "18        0.666667     6.280479  \n",
       "19        0.666667     6.264108  \n",
       "20        0.677083     6.229817  \n",
       "21        0.666667     6.350921  \n",
       "22        0.638889     6.330600  \n",
       "23        0.649306     6.322739  \n",
       "24        0.656250     6.288002  \n",
       "25        0.659722     6.286578  \n",
       "26        0.635417     6.360933  \n",
       "27        0.621528     6.305207  \n",
       "28        0.638889     6.271463  \n",
       "29        0.642361     6.292075  \n",
       "..             ...          ...  \n",
       "94        0.472222    10.142100  \n",
       "95        0.465278    10.321460  \n",
       "96        0.451389    10.429427  \n",
       "97        0.451389    10.275648  \n",
       "98        0.402778    10.308085  \n",
       "99        0.427083    10.306196  \n",
       "100       0.388889    10.195339  \n",
       "101       0.409722    10.387146  \n",
       "102       0.427083    10.207377  \n",
       "103       0.416667    10.324717  \n",
       "104       0.409722    10.300213  \n",
       "105       0.427083    10.239567  \n",
       "106       0.385417    10.293681  \n",
       "107       0.413194    10.267694  \n",
       "108       0.409722    10.140549  \n",
       "109       0.409722    10.221695  \n",
       "110       0.381944    10.227983  \n",
       "111       0.364583    10.172771  \n",
       "112       0.402778    10.241439  \n",
       "113       0.416667    10.352921  \n",
       "114       0.416667    10.453943  \n",
       "115       0.409722    10.337733  \n",
       "116       0.434028    10.259551  \n",
       "117       0.413194    10.325859  \n",
       "118       0.395833    10.394575  \n",
       "119       0.399306    10.234006  \n",
       "120       0.385417    10.217508  \n",
       "121       0.427083    10.330666  \n",
       "122       0.392361  1721.873848  \n",
       "123       0.319444    40.783800  \n",
       "\n",
       "[124 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
